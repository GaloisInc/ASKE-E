The Problem

Propose a Level of Effort (LOE) metric to quantify the amount of programming work required to develop a correct implementation of a simulation of an epidemiological model.

The Initial Proposal

We propose that LOE is related to program complexity, which in turn is related primarily to the size of the program. To a first approximation, a longer program takes longer to write. The challenge then becomes finding a way to quantify the length of a program in a way that is somewhat predictable despite variations in programmer style and preference, for example the presence or absence of comments and whitespace and use of short versus long identifier names.

The Kolmogorov complexity[1] metric suggests a solution which is independent of choice of representation: find the shortest program which emits the text of the program to be measured. While Kolmogorov complexity is not computable[2], compressibility of the program text serves as an approximate upper bound on the vale expected of the the text's Kolmogorov complexity[3,4].

Caveats

Kolmogorov complexity (and our computable proxy: compressibility) considers the entire program text, including comments (which are informative to the reader, but do not contribute to the behavior of the program) and translation directives of various kinds (e.g. compiler directives, pragmas and makefiles). We should always strip comments. It's less clear what to do about translation directives; we will attend to this question after discussions about whether our proposal satisifies the desire for an LOE metric.

Alpha-conversion

When using compressibility as a proxy for Kolmogorov complexity, the resulting metric is affected strongly affected by identifier length. We address this by renaming user-defined symbols to have the minimum number of characters needed, based upon the number of such symbols in the program. Likewise, while literal strings are essential to the proper function of a program, they do not (absent use of an eval-like function) contribute the the essential complexity of a program; we normalize all strings to a fixed length and content. Symbols which are essential to describe the interpretation of a program, such as library function names and reserved words in the language, are not subjected to alpha-conversion.

Other Considerations

The implementation of the compression method used as a proxy for Kolmogorov complexity will affect the compressibility metric. This will be particularly true in the case of short program texts, as the size of the dictionary (which is part of the compressed text) may, for example, be padded to some minimum size for the convenience of the decompression algorithm. Additionally, some compression methods are known to emit multiple dictionaries; this can happen when the compressor encounters new text in the input stream and that text is not adequately compressible using the active dictionary[5]. It's not clear how or whether this will affect the compressibility measurement.

Implementations of a given compression method may differ in their encodings while still being considered to be the same method[6]; we don't know how significant these differences may be as regards a compressibility measure.

Other Approaches

We attempted to apply Latent Semantic Indexing[7] as a measure of document similarity. We thought of this not as LOE for a specific program text, but rather as a measure of the effort required to transform one text into a larger, more complex text (e.g. ODE to RNET or RNET to a discrete event simulation written in a general-purpose programming language). We quickly discovered that the method is not useful when scaled down to using a single document as a corpus.

We considered and rejected the use of edit distance[8] (e.g. Levenshtein distance[9]) to measure document similiarity. The edit distance is sensitive to spelling variations; transforming a program by simply renaming all of its identifiers will result in a nonzero edit distance even though the two variations are functionally identical. 

As an adjunct to compressibility, we also count the number of unique tokens (excluding punctuation) in a program text as an indicator of cognitive effort for the programmer; the intuition is that a larger number of unique tokens contributes to an increased LOE.

Results

We collected from the Rosetta Code website[10] a number of sample programs written in C, C++, FORTRAN and Haskell. Some of these samples have been edited for consistency across languages; e.g. to remove test drivers and data leaving only the essential algorithmic code. We also gathered several ODEs with corresponding solvers written in Julia[11] and Python[12]. Comments have been removed from all source texts.


The following table shows the analysis of the sample source codes.

    Source text        wc -lc     K(s) estimate  Unique symbols       K(s)/U(s)
        hello.c       5      52              55               6            9.16
      hello.cpp       7      86              83              10            8.30
      hello.f90       4      69              69               7            9.85
       hello.hs       1      25              29               3            9.66
    quicksort.c      20     346             213              21           10.14
  quicksort.cpp      22     673             293              23           12.73
  quicksort.f90      59    1166             585              38           15.39
   quicksort.hs       2      94              77               8            9.62
     toposort.c      64    1338             731              52           14.05
   toposort.cpp      59    2037             739              57           12.96
   toposort.f90      37     666             409              30           13.63
    toposort.hs      23     757             515              56            9.19
    ackermann.c       6     160             101               7           14.42
  ackermann.cpp       9     197             131               9           14.55
  ackermann.f90      11     268             173              14           12.35
   ackermann.hs       4     107              88               8           11.00
      ode-1.txt       1      48              55               8            6.87
       ode-1.jl      11     235             204              25            8.16
     ode-1_1.py      15     358             267              35            7.62
     ode-1_2.py      16     444             311              32            9.71
     ode-1_3.py      14     303             231              28            8.25
     ode-1_4.py      14     313             243              29            8.37
     ode-1_5.py      27     801             569              63            9.03
      ode-2.txt       5      85              78              12            6.50
       ode-2.jl      20     407             314              44            7.13
     ode-2_1.py      23     471             320              47            6.80
     ode-2_2.py      22     481             353              42            8.40
     ode-2_3.py      21     434             300              42            7.14
     ode-2_4.py      23     449             333              44            7.56
     ode-2_5.py      41     979             663              73            9.08
      ode-3.txt       3      47              50               8            6.25
       ode-3.jl      15     308             248              33            7.51
     ode-3_1.py      21     438             301              44            6.84
     ode-3_2.py      21     478             350              41            8.53
     ode-3_3.py      19     446             293              41            7.14
     ode-3_4.py      22     437             311              43            7.23
     ode-3_5.py      27     801             569              63            9.03

References

[1] https://en.wikipedia.org/wiki/Kolmogorov_complexity
[2] https://en.wikipedia.org/wiki/Kolmogorov_complexity#Uncomputability_of_Kolmogorov_complexity
[3] https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression
[4] https://en.wikipedia.org/wiki/Lempel-Ziv_complexity
[5] ? - streaming compression dictionary update
[6] https://en.wikipedia.org/wiki/LZ77_and_LZ78
[7] https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing
[8] https://en.wikipedia.org/wiki/Edit_distance
[9] https://en.wikipedia.org/wiki/Levenshtein_distance
[10] https://rosettacode.org/
[11] https://computationalmindset.com/en/neural-networks/ordinary-differential-equation-solvers-in-julia.html
[12] https://computationalmindset.com/en/neural-networks/ordinary-differential-equation-solvers.html
